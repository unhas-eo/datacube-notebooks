{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>UNHAS: Using a shapefile to mask raster data</u>\n",
    "\n",
    "**Contents**\n",
    "\n",
    "  - [Overview](#Overview)\n",
    "  - [Notebook setup](#Notebook-setup)\n",
    "  - [Loading up the Sentinel-2 data](#Loading-up-the-Sentinel-2-data)\n",
    "    - [DataCube query](#DataCube-query)\n",
    "    - [Data load](#Data-load)\n",
    "    - [Cleaning up the data](#Cleaning-up-the-data)\n",
    "    - [Time series display](#Time-series-display)\n",
    "  - [Masking the raster data with a shapefile](#Masking-the-raster-data-with-a-shapefile)\n",
    "    - [Loading up the shapefile](#Loading-up-the-shapefile)\n",
    "    - [Creating a raster mask](#Creating-a-raster-mask)\n",
    "    - [Applying the mask to the data time series](#Applying-the-mask-to-the-data-time-series)\n",
    "    \n",
    "\n",
    "# Overview\n",
    "\n",
    "This notebook demonstrates how to use a shapefile to mask raster data extracted from the EASI database. For demonstration purposes, the notebook makes use of Sentinel-2 data acquired over a region of interest centred on Lake Tempe, Indonesia, and uses an existing shapefile containing the polygon(s) to apply as a mask. This shapefile is available in the `ancillary_data` folder in this repository.\n",
    "\n",
    "This notebook is adapted from a [Digital Earth Australia](https://github.com/GeoscienceAustralia/dea-notebooks) example by Claire Krause.\n",
    "\n",
    "**A note regarding compatibility**\n",
    "\n",
    "As stated above, this notebook makes use of a specific dataset of remote sensing data, and is applied over a given region of interest. The code below will thus not run properly if the EASI deployment used to run this notebook does not also contain a similar dataset.\n",
    "\n",
    "Users can nonetheless investigate the outputs provided in this demonstration notebook, and also potentially modify certain variables in the code below to allow the notebook to run with a different EASI deployment (e.g. different remote sensing data, region of interest, etc.)\n",
    "\n",
    "\n",
    "# Notebook setup\n",
    "\n",
    "Here, we import the relevant Python modules and functions needed in the rest of this notebook. Subsequently, we open a connection to the EASI datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System\n",
    "import os, sys\n",
    "\n",
    "### Datacube \n",
    "from datacube import Datacube\n",
    "from datacube.utils import masking\n",
    "from odc.algo import enum_to_bool\n",
    "\n",
    "### Data tools\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import geopandas as gpd\n",
    "\n",
    "### Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### EASI tools\n",
    "sys.path.append('/home/jovyan/git_hub_notebook/scripts/')   \n",
    "   # 'scripts' available by cloning this repository: https://dev.azure.com/csiro-easi/easi-hub-public/_git/hub-notebooks\n",
    "import notebook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:24.010171Z",
     "start_time": "2018-08-29T05:35:17.970519Z"
    }
   },
   "outputs": [],
   "source": [
    "dc = Datacube(app='Raster mask from shapefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading up the Sentinel-2 data\n",
    "\n",
    "## DataCube query\n",
    "\n",
    "In this notebook, we're interested in a dataset of Sentinel-2 imagery (product labelled `s2_l2a` on the current EASI deployment) centred over the Lake Tempe region. We set up the corresponding spatial extent in the `query` dicitionary below, together with a (small) temporal window. \n",
    "\n",
    "We will use the Sentinel-2 data for false-colour plots of the SWIR, NIR and green bands (in the corresponding RGB channels). We thus add these bands to the list of `measurements` to load up, together with the `SCL / mask / qa` band of pixel quality data, which we will use to remove bad-quality pixels.\n",
    "\n",
    "We will also plot the dataset in its original (native) projection, so in the code below, we use the function `mostcommon_crs()` to work out the dataset's native coordinate reference system (CRS) and set the query parameter `output_crs` to that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = { 'product': 's2_l2a',\n",
    "          'lat': (-3.89, -4.29),           # Lake Tempe\n",
    "          'lon': (119.75, 120.15),\n",
    "          'measurements': ['green', 'nir_1', 'swir_2', 'qa'],\n",
    "          'time': ('2020-01-01', '2020-03-01'),\n",
    "          'group_by': 'solar_day' }        # scene ordering\n",
    "\n",
    "### Dataset's native projection\n",
    "native_crs = notebook_utils.mostcommon_crs(dc, query)\n",
    "print(f\"The dataset's native CRS is: {native_crs}\")\n",
    "\n",
    "query.update({ 'output_crs': native_crs,   # EPSG code\n",
    "               'resolution': (30, 30) })   # target resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dc.load(**query)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our dataset is here relatively small, we did not make use of `Dask` (distributed `Xarray` processing on compute clusters). As usual, we can check the size of the data object we just loaded up into the system memory &ndash; using a very large dataset (without `Dask`) could lead to technical and/or computational issues in the code further below in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The size of 'data' in memory is {data.nbytes/(1024**2):.2f} MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the data\n",
    "\n",
    "Once loaded, we typically want to (pre-)process the data to remove pixels flagged as `nodata`, as well as those pixels affected by specific data issues such as cloud, cloud shadow, saturation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Valid mask (i.e. not 'nodata'), for each data layer\n",
    "valid_mask = masking.valid_data_mask(data)\n",
    "\n",
    "### Mask of valid pixels\n",
    "good_pixel_flags = {'vegetation', 'bare soils', 'unclassified', 'water'}   # pixels to retain (i.e. remove pixels flagged as 'nodata', 'cloud', 'saturated', etc.)\n",
    "good_pixel_mask = enum_to_bool(data['qa'], good_pixel_flags)  # -> DataArray\n",
    "\n",
    "### Scaling factors for Sentinel-2 data (GA S2 products)\n",
    "scale = 0.0001  # divide by 10000\n",
    "offset = 0.0\n",
    "\n",
    "### Apply valid mask, good pixel mask, and scaling to each layer\n",
    "data['swir_2'] = data['swir_2'].where(valid_mask['swir_2'] & good_pixel_mask) * scale + offset\n",
    "data['nir_1'] = data['nir_1'].where(valid_mask['nir_1'] & good_pixel_mask) * scale + offset\n",
    "data['green'] = data['green'].where(valid_mask['green'] & good_pixel_mask) * scale + offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series display\n",
    "\n",
    "For demonstration purposes, we can also select a small number of time slice to display from the original time series of 11 images. Below, we identify the four time indices that contain the largest numbers of valid pixels, and extract these time slices from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clear_pix = good_pixel_mask.sum(['x','y'])   # number of valid pixels, in each time slice\n",
    "pix_thr = np.sort( num_clear_pix )[-4]     # sort the time series and select a threshold (4th largest number)\n",
    "clearest_ind = (num_clear_pix>=pix_thr)    # indices of time slices with nr of pixels above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract time slices from the time series\n",
    "data = data.isel(time=clearest_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the resulting time series (false-colour display)\n",
    "image_array = data[['swir_2', 'nir_1', 'green']].to_array()\n",
    "image_array.plot.imshow(robust=True, col='time', col_wrap=2, size=6, aspect=data.x.shape[0]/data.y.shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking the raster data with a shapefile\n",
    "\n",
    "## Loading up the shapefile\n",
    "\n",
    "For this example, we use a polygon of the Lake Tempe boundary line, which can be accessed from the shapefile provided in the `ancillary_data` folder in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_file = './ancillary_data/Base Map/Boundary_administration.shp'\n",
    "\n",
    "### Load the shapefile\n",
    "shp = gpd.read_file(shape_file)\n",
    "display(shp)\n",
    "shp.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the vector data within that shapefile is in the projection `EPSG:4326`, which is different from that of our main Sentinel-2 dataset (`EPSG:32750`). For compatibility, we can here re-project the shapefile data to the CRS of the Sentinel-2 dataset. \n",
    "\n",
    "Subsequently, we will also filter the shapefile contents to only select those polygons associated with Lake Tempe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reproject to current coordinate reference system\n",
    "shp = shp.to_crs(native_crs)\n",
    "\n",
    "### Remove unwanted polygons\n",
    "drop_list = []\n",
    "for ff in shp.iterrows():\n",
    "    if 'tempe' not in ff[1].Village.lower():\n",
    "        drop_list.append( ff[0] )\n",
    "        \n",
    "shp.drop( drop_list, inplace=True )\n",
    "\n",
    "### Plot\n",
    "shp.boundary.plot(figsize=(8,8))\n",
    "plt.xlabel(\"x [metre]\"); plt.ylabel(\"y [metre]\")\n",
    "plt.title(\"Lake Tempe boundary\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a raster mask\n",
    "\n",
    "We can now create a raster mask from the vector data. The code below iterates over the polygons in the shapefile (in case multiple polygons are available), setting the raster mask values to `1` for all the pixels corresponding to the footprint of each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:29.830439Z",
     "start_time": "2018-08-29T05:35:29.673975Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = rasterio.features.rasterize( ((feature['geometry'], 1) for feature in shp.iterfeatures()),\n",
    "                                    out_shape = (data.dims['y'],data.dims['x']),\n",
    "                                    transform = data.affine )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:29.838006Z",
     "start_time": "2018-08-29T05:35:29.833034Z"
    }
   },
   "outputs": [],
   "source": [
    "### Convert the mask (numpy array) to an Xarray DataArray\n",
    "mask = xr.DataArray(mask, coords=(data.y, data.x))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:29.938180Z",
     "start_time": "2018-08-29T05:35:29.840231Z"
    }
   },
   "outputs": [],
   "source": [
    "mask.plot(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the mask to the data time series\n",
    "\n",
    "Finally, we can use the mask we just created, apply it to the time series of Sentinel-2 data, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Masking\n",
    "data = data.where(mask)\n",
    "\n",
    "### Plotting\n",
    "image_array = data[['swir_2', 'nir_1', 'green']].to_array()\n",
    "image_array.plot.imshow(robust=True, col='time', col_wrap=2, size=6, aspect=data.x.shape[0]/data.y.shape[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-29T05:35:30.027851Z",
     "start_time": "2018-08-29T05:35:29.970230Z"
    }
   },
   "outputs": [],
   "source": [
    "### End notebook"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
