{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# UNHAS: Monitoring chlorophyll-<i>a</i> in Lake Tempe\n",
    "\n",
    "<!-- <i><font color='red'>Notebook & repo</font></i> -->\n",
    "\n",
    "**Contents**\n",
    "\n",
    " - [Overview](#Overview)\n",
    "   - [Sentinel-2 use case](#Sentinel-2-use-case)\n",
    "   - [Description](#Description)\n",
    " - [Notebook setup](#Notebook-setup)\n",
    "   - [Dask](#Dask)\n",
    " - [Region of interest](#Region-of-interest)\n",
    "   - [Analysis parameters](#Analysis-parameters)\n",
    "   - [Selected location display](#Selected-location-display)\n",
    " - [Sentinel-2 dataset](#Sentinel-2-dataset)\n",
    "   - [Interrogating the DC database](#Interrogating-the-DC-database)\n",
    "   - [Loading the data](#Loading-the-data)\n",
    "   - [Cleaning up the data](#Cleaning-up-the-data)\n",
    "   - [Visualise the data](#Visualise-the-data)\n",
    " - [Masking the raster data with a shapefile](#Masking-the-raster-data-with-a-shapefile)\n",
    "   - [Loading up the shapefile](#Loading-up-the-shapefile)\n",
    "   - [Creating a raster mask](#Creating-a-raster-mask)\n",
    "   - [Applying the mask to the data time series](#Applying-the-mask-to-the-data-time-series)\n",
    " - [Band indices](#Band-indices)\n",
    "   - [Band arithmetic](#Band-arithmetic)\n",
    "   - [Computation](#Computation)\n",
    " - [Summary plot](#Summary-plot)\n",
    "   - [Analysis constants](#Analysis-constants)\n",
    "   - [Total water area](#Total-water-area)\n",
    "   - [Average NDCI](#Average-NDCI)\n",
    "   - [Combined indices plot](#Combined-indices-plot)\n",
    " - [Spatial plots](#Spatial-plots)\n",
    "   - [NDCI plots](#NDCI-plots)\n",
    "   - [True colour display](#True-colour-display)\n",
    "\n",
    "# Overview\n",
    "\n",
    "Inland waterbodies are essential for supporting human life, both through the supply of drinking water and the support of agriculture and aquaculture.\n",
    "Such waterbodies can be contaminated by outbreaks of blue-green (and other toxic) algae, causing health issues for people and animals.\n",
    "For example, up to a million fish died during an [algal bloom event](https://www.abc.net.au/news/2019-01-08/second-fish-kill-in-darling-river-at-menindee/10696632) in the Darling river in late 2018 and early 2019. \n",
    "While the health of waterbodies can be monitored from the ground through sampling, satellite imagery can complement this, potentially improving the detection of large algal bloom events.\n",
    "However, there needs to be a well-understood and tested way to link satellite observations to the presence of algal blooms.\n",
    "\n",
    "## Sentinel-2 use case\n",
    "\n",
    "<!-- <i><font color='red'>NDCI and NDWI</font></i> -->\n",
    "\n",
    "Algal blooms are associated with the presence of clorophyll-*a* in waterbodies.\n",
    "[Mishra and Mishra (2012)](https://doi.org/10.1016/j.rse.2011.10.016) developed the normalised difference chlorophyll index (NDCI), which serves as a qualitative indicator for the concentration of chlorophyll-*a* on the surface of a waterbody.\n",
    "The index requires information from a specific part of the infrared specturm, known as the 'red edge'. \n",
    "This is captured as part of Sentinel-2's spectral bands, making it possible to measure the NDCI with Sentinel-2. \n",
    "\n",
    "On the other hand, the following caveats should also be kept in mind:\n",
    "\n",
    "* The NDCI is currently treated as an experimental index for Australia, as futher work is needed to calibrate and validate how well the index relates to the presence of chlorophyll-*a*. \n",
    "* It is also important to remember that algal blooms will usually result in increased values of the NDCI, but not all NDCI increases will be from algal blooms. For example, there may be seasonal fluctuations in the amount of chlorophyll-*a* in a waterbody.\n",
    "* Further validation work is required to understand how shallow water and atmospheric effects affect the NDCI, and its use in identifying high concentrations of chlorophyll-*a*.\n",
    "\n",
    "## Description\n",
    "\n",
    "In this example, we measure the NDCI for Lake Tempe, South Sulawesi, Indonesia. This is combined with information about the size of the waterbody, which is used to build a helpful visualisation of how the water level and presence of chlorophyll-*a* changes over time. The worked example takes users through the code required to:\n",
    "\n",
    "<!-- <i><font color='red'>Overview</font></i> -->\n",
    "\n",
    "1. Interrogating the EASI / ODC database for products (satellites) and measurements (satellite bands and derived products)\n",
    "1. Use of Dask for distributed processing of large datasets\n",
    "1. Load Sentinel-2 images for the area of interest and clean up the imagery (clouds, 'no-data' pixels, etc.)\n",
    "1. Using a shape file to mask the dataset\n",
    "1. Compute indices to measure the presence of water and chlorophyll-*a*\n",
    "1. Generate informative visualisations to identify the presence of chlorophyll-*a*.\n",
    "\n",
    "This notebook is adapted from a [Digital Earth Australia](https://github.com/GeoscienceAustralia/dea-notebooks) example, and can be found in the following [GitHub](https://github.com/unhas-eo/datacube-notebooks) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook setup\n",
    "\n",
    "Here, we load the key Python packages and supporting functions for the subsequent analysis. \n",
    "\n",
    "> The EASI Tools section comes from https://dev.azure.com/csiro-easi/easi-hub-public/_git/hub-notebooks. Git clone this repository to use these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### System\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### Datacube \n",
    "import datacube\n",
    "from datacube.utils import masking\n",
    "from odc.algo import enum_to_bool\n",
    "\n",
    "### Data tools\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import rasterio.features\n",
    "\n",
    "### Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### EASI tools\n",
    "sys.path.append('/home/jovyan/git_hub_notebook/scripts/')\n",
    "   # 'scripts' available by cloning this repository: https://dev.azure.com/csiro-easi/easi-hub-public/_git/hub-notebooks\n",
    "from app_utils import display_map\n",
    "import notebook_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's now connect to the datacube database, which provides functionality for loading and displaying stored Earth observation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Chlorophyll_monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "Rather than loading the Sentinel-2 data into the system memory, this notebook will make use of Dask to load up and process the dataset.\n",
    "\n",
    "Dask is a Python library for distributed (parallel) processing, which allows us to work on very large array or datacube objects that would otherwise not fit in the memory of any single compute node. Dask relies on the creation of a Dask Gateway (or local) cluster with associated scheduler, which is done in the following cell through the `initialize_dask` helper function.\n",
    "\n",
    "<!-- <i><font color='red'>Dask cluster and scheduler</font></i> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster, client = notebook_utils.initialize_dask(workers=2, use_gateway=True, wait=False)\n",
    "display(cluster if cluster else client)\n",
    "notebook_utils.localcluster_dashboard(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dashboard` link provided above (`Client` section) can be used to monitor the status of the Dask cluster and associated processing tasks during the execution of various cells in the rest of this notebook.\n",
    "\n",
    "# Region of interest\n",
    "\n",
    "## Analysis parameters\n",
    "\n",
    "The following cell sets the analysis parameters, which define the area of interest and the length of time to conduct the analysis over.\n",
    "\n",
    "<!-- <i><font color='red'>Large dataset</font></i> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Area of interest (Lake Tempe)\n",
    "latitude = (-4.03, -4.2)\n",
    "longitude = (119.87, 120.03)\n",
    "\n",
    "### Range of dates for the analysis\n",
    "time = (\"2018-01-01\", \"2022-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Selected location display\n",
    "\n",
    "The next cell will display the selected area on an interactive map. Feel free to zoom in and out to get a better understanding of the area you'll be analysing. Clicking on any point of the map will reveal the latitude and longitude coordinates of that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_map(x=longitude, y=latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 dataset\n",
    "\n",
    "## Interrogating the DC database\n",
    "\n",
    "We can obtain a summary of the various data products available from the current ODC / EASI database, as follows.\n",
    "\n",
    "<!-- <i><font color='red'>Products, measurements and explorer</font></i> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.list_products()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to investigate the available datasets is through the [ODC Explorer](https://explorer.sg-dev.easi-eo.solutions/products/s2_l2a/extents) interface for the current EASI deployment.\n",
    "\n",
    "In this notebook, we will make use of the dataset of Sentinel-2 imagery, labelled `s2_l2a`. We can further interrogate the specific bands (measurements) available within this product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_measurements = dc.list_measurements().loc[\"s2_l2a\"]\n",
    "product_measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further below, we will mask the data to identify the pixels containing valid information (i.e. not `no-data`), as well as those pixels not affected by clouds, cloud shadow, etc. (i.e. clear pixels). Information regarding the quality of each pixel is provided in the `SCL` data layer (with `mask` or `qa` aliases), which can be investigated in more detail as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_measurements.loc['SCL']['flags_definition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "The first step in the analysis is to load Sentinel-2 data for the specified area of interest and time range. We also specifically select various spectral bands to load: the `red`, `green` and `blue` bands will allow for true-colour displays of the Sentinel-2 data, while other bands will be used for the calculation of water-related indices further below. In addition, the pixel QA band `fmask` will be used to mask out the pixels affected by clounds, shadows, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datacube load query\n",
    "query = { \"product\": \"s2_l2a\",\n",
    "          \"x\": longitude,\n",
    "          \"y\": latitude,\n",
    "          \"time\": time,\n",
    "          \"measurements\": [ \"red_edge_1\",  # Red edge 1 band\n",
    "                            \"red\",         # Red band\n",
    "                            \"green\",       # Green band\n",
    "                            \"blue\",        # Blue band\n",
    "                            \"swir_2\",      # Short-wave infrared band\n",
    "                            \"mask\" ],           # Pixel QA band\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinel-2 datasets are stored with different coordinate reference systems (CRS), corresponding to multiple UTM zones used for S2 L1B tiling. S2 measurement bands also have different resolutions (10 m, 20 m and 60 m). As such S2 queries need to include the following two query parameters:\n",
    "\n",
    "- `output_crs` - This sets a consistent CRS that all Sentinel-2 data will be reprojected to, irrespective of the UTM zone the individual image is stored in.\n",
    "- `resolution` - This sets the resolution that all Sentinel-2 images will be resampled to.\n",
    "\n",
    "Use `mostcommon_crs()` to select a CRS. Adapted from https://github.com/GeoscienceAustralia/dea-notebooks/blob/develop/Tools/dea_tools/datahandling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which is the most common native CRS\n",
    "native_crs = notebook_utils.mostcommon_crs(dc, query)\n",
    "display(f'Native CRS: {native_crs}')\n",
    "\n",
    "query.update({\n",
    "    \"output_crs\": native_crs,\n",
    "    \"resolution\": (-20, 20),\n",
    "    \"dask_chunks\": {\"y\":2048, \"x\":2048},\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <i><font color='red'>Datacube load into Dask arrays</font></i> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2 = dc.load(**query)\n",
    "ds_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the data\n",
    "\n",
    "Next, we will mask out any clouds (and other pixel QA issues) in the dataset, and also filter out any image where less than 70% of the pixels are classified as clear observations. Note here that we're including `snow` pixels in the dataset of \"good\" pixels &ndash; quite a few pixels in the time series are identified as being `snow`, but snow cover is highly improbable at the selected location. Instead, these `snow` pixels are very likely to actually represent clear observations that are mis-classified, which is likely to occur in salt lake regions with highly reflective land covers. Rather than removing these likely mis-classified pixels, we thus include them in the set of clear observations.\n",
    "\n",
    "<!-- <i><font color='red'>Removing invalid and low-quality pixels, low-quality images</font></i> -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gooddata = 0.6   # percentage threshold\n",
    "\n",
    "### Mask of valid pixels\n",
    "good_pixel_flags = {'bare soils','water','vegetation',                   # pixels to retain (i.e. remove pixels flagged as 'no data','saturated or defective',\n",
    "                    'snow or ice','dark area pixels','unclassified'}     # 'cloud shadows','cloud medium probability','cloud high probability','thin cirrus')\n",
    "\n",
    "good_pixel_mask = enum_to_bool(ds_s2['mask'], good_pixel_flags)\n",
    "good_pixel_mask = good_pixel_mask.persist()\n",
    "\n",
    "### Percentage of good data for each time slice\n",
    "data_perc = good_pixel_mask.sum(axis=[1,2]) / (good_pixel_mask.shape[1]*good_pixel_mask.shape[2])\n",
    "keep = (data_perc>=min_gooddata).persist()\n",
    "\n",
    "### Drop low quality time slices\n",
    "total_obs = len(ds_s2.time)\n",
    "ds_s2 = ds_s2.sel(time=keep)\n",
    "good_pixel_mask = good_pixel_mask.sel(time=keep)\n",
    "\n",
    "print(f'Filtered the time series to {len(ds_s2.time)} (out of {total_obs}) '\n",
    "      f'time steps with at least {min_gooddata:.1%} good quality pixels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply the resulting 'good pixel' mask to the data, as well as a further 'no-data' mask and further scaling operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Valid mask (i.e. not 'nodata'), for each data layer\n",
    "valid_mask = masking.valid_data_mask(ds_s2)\n",
    "\n",
    "### Scaling factors for Sentinel-2 data (GA S2 products)\n",
    "scale = 0.0001  # divide by 10000\n",
    "offset = 0.0\n",
    "\n",
    "### Apply valid mask, good pixel mask, and scaling to each layer\n",
    "ds_s2['red_edge_1'] = ds_s2['red_edge_1'].where(valid_mask['red_edge_1'] & good_pixel_mask) * scale + offset\n",
    "ds_s2['red'] =        ds_s2['red'].where(valid_mask['red'] & good_pixel_mask)               * scale + offset\n",
    "ds_s2['green'] =      ds_s2['green'].where(valid_mask['green'] & good_pixel_mask)           * scale + offset\n",
    "ds_s2['blue'] =       ds_s2['blue'].where(valid_mask['blue'] & good_pixel_mask)             * scale + offset\n",
    "ds_s2['swir_2'] =     ds_s2['swir_2'].where(valid_mask['swir_2'] & good_pixel_mask)         * scale + offset\n",
    "ds_s2 = ds_s2.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the processing is complete, we can examine the data as done in the next cell. The `Dimensions` argument reveals the number of time steps in the dataset, as well as the number of pixels in the `x` (longitude) and `y` (latitude) dimensions. Clicking on the `data repr` icon for each of the `Data variables` also confirms that the data is currently loaded as Dask chunks on the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the data\n",
    "\n",
    "We can visualise the data using a true-colour image display for a number of time steps. White areas indicate where clouds or other invalid pixels in the image have been masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.linspace(1, ds_s2.sizes['time'], 3, dtype='int') - 1   # select some time slices to display\n",
    "\n",
    "### Plot the selected time slices (true-colour display)\n",
    "image_array = ds_s2[['red', 'green', 'blue']].isel(time=tmp).to_array()\n",
    "image_array.plot.imshow(robust=True, col='time', col_wrap=3, size=6, aspect=ds_s2.x.shape[0]/ds_s2.y.shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking the raster data with a shapefile\n",
    "\n",
    "## Loading up the shapefile\n",
    "\n",
    "For this example, we use a polygon of Lake Tempe's boundary, which is provided in the `ancillary_data` folder in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_file = './ancillary_data/Base Map/Boundary_CA_TempeLake_AR.shp'\n",
    "shape_file = './ancillary_data/Base Map/Boundary_administration.shp'\n",
    "\n",
    "### Load the shapefile\n",
    "shp = gpd.read_file(shape_file)\n",
    "display(shp)\n",
    "shp.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the vector data within that shapefile is in the projection `EPSG:4326`, which is different from that of our main Sentinel-2 dataset (`EPSG:32750`). For compatibility, we can here re-project the shapefile data to the CRS of the Sentinel-2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reproject\n",
    "shp = shp.to_crs(native_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique( list(shp.Province) )\n",
    "# np.unique( list(shp.Regency) )\n",
    "# np.unique( list(shp.District) )\n",
    "# np.unique( list(shp.Village) )\n",
    "# [ii for ii,vv in enumerate(list(shp.Village)) if 'tempe' in vv.lower()]\n",
    "# [ii for ii,vv in enumerate(list(shp.District)) if 'tempe' in vv.lower()]\n",
    "# [ii for ii,vv in enumerate(list(shp.Regency)) if 'tempe' in vv.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape file we've just loaded contains a large number of polygons corresponding to the various jurisdictions in South Sulawesi. So let's now select only those polygons corresponding to Lake Tempe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_list = []\n",
    "for ff in shp.iterrows():\n",
    "    if 'tempe' in ff[1].Village.lower():\n",
    "        print(ff[0],ff[1].Village)\n",
    "    else:\n",
    "        drop_list.append( ff[0] )\n",
    "        \n",
    "shp.drop( drop_list, inplace=True )\n",
    "shp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now produce a plot of the polygons of interest from the original shape file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot\n",
    "shp.boundary.plot(figsize=(8,8))\n",
    "plt.xlabel(\"x [metre]\"); plt.ylabel(\"y [metre]\")\n",
    "plt.title(\"Lake Tempe boundary\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a raster mask\n",
    "\n",
    "We can now create a raster mask from the vector data. The code below iterates over the polygons in the shapefile (in case multiple polygons are available), setting the raster mask values to 1 for all the pixels corresponding to the footprint of each polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = rasterio.features.rasterize( ((feature['geometry'], 1) for feature in shp.iterfeatures()),\n",
    "                                    out_shape = (ds_s2.dims['y'],ds_s2.dims['x']),\n",
    "                                    transform = ds_s2.affine )\n",
    "\n",
    "### Convert the mask (numpy array) to an Xarray DataArray\n",
    "mask = xr.DataArray(mask, coords=(ds_s2.y, ds_s2.x))\n",
    "\n",
    "mask.plot(figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the mask to the data time series\n",
    "\n",
    "Finally, we can use the mask we just created, apply it to the time series of Sentinel-2 data, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Masking\n",
    "ds_s2 = ds_s2.where(mask)\n",
    "ds_s2 = ds_s2.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.linspace(1, ds_s2.sizes['time'], 3, dtype='int') - 1   # select some time slices to display\n",
    "\n",
    "### Plot the selected time slices (true-colour display)\n",
    "image_array = ds_s2[['red', 'green', 'blue']].isel(time=tmp).to_array()\n",
    "image_array.plot.imshow(robust=True, col='time', col_wrap=3, size=6, aspect=ds_s2.x.shape[0]/ds_s2.y.shape[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Band indices\n",
    "\n",
    "## Band arithmetic\n",
    "\n",
    "This study measures the presence of water through the modified normalised difference water index (MNDWI), while the amount of clorophyll-*a* is calculated using the normalised difference clorophyll index (NDCI).\n",
    "\n",
    "MNDWI is calculated from the `green` and shortwave infrared (`SWIR`) bands to identify water.\n",
    "The formula is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{MNDWI} = \\frac{\\text{Green} - \\text{SWIR}}{\\text{Green} + \\text{SWIR}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When interpreting this index, values greater than 0 indicate water.\n",
    "\n",
    "NDCI is calculated from the `red edge 1` and `red` bands to identify water.\n",
    "The formula is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{NDCI} = \\frac{\\text{Red_edge_1} - \\text{Red}}{\\text{Red_edge_1} + \\text{Red}}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "When interpreting this index, higher values indicate a higher concentration of clorophyll-*a*.\n",
    "\n",
    "## Computation\n",
    "\n",
    "As per the formulae provided above, we can now calculate the relevant indices from the various bands in our dataset. The code below achieves this, and subsequently saves the results back as new layers in the `ds_s2` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNDWI\n",
    "ds_s2['MNDWI'] = ( (ds_s2.green - ds_s2.swir_2) /\n",
    "                   (ds_s2.green + ds_s2.swir_2) )\n",
    "\n",
    "### NDCI\n",
    "ds_s2['NDCI'] = ( (ds_s2.red_edge_1 - ds_s2.red) /\n",
    "                  (ds_s2.red_edge_1 + ds_s2.red) )\n",
    "\n",
    "ds_s2 = ds_s2.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNDWI and NDCI values should now appear as data variables, along with the loaded measurements, in the `ds_s2` data set. We can check this by printing the data set below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary plot\n",
    "\n",
    "To get an understanding of how the waterbody has changed over time, the following section builds a plot that uses the MNDWI to measure (roughly) the area of the waterbody, along with the NDCI to track how the concentration of clorophyll-*a* is changing over time. This could be used to quickly assess the status of a given waterbody."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis constants\n",
    "\n",
    "If the pixel area is known, the number of pixels classified as water (i.e, where MNDWI > 0) can be used as a proxy for the area of the waterbody. The following cell generates the necessary constants for performing this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_length = query[\"resolution\"][1]   # in metres\n",
    "m_per_km = 1000   # conversion from metres to kilometres\n",
    "\n",
    "area_per_pixel = pixel_length**2 / m_per_km**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total water area\n",
    "\n",
    "The next cell starts by filtering the dataset to only keep the pixels that are classified as water. It then calculates the water area by counting all the MNDWI pixels in the filtered data set, calculating a rolling median &ndash; this helps smooth the results to account for variation from cloud-masking. This median count is then multiplied by the area-per-pixel value to obtain an estimate of the water area over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Water pixels\n",
    "ds_s2_water = ds_s2.where(ds_s2.MNDWI>0.0)\n",
    "ds_s2_watercount = ds_s2_water.MNDWI.count(dim=[\"x\", \"y\"])\n",
    "\n",
    "### Total water area\n",
    "waterarea = ds_s2_watercount.rolling(time=3, center=True, min_periods=1).median(skipna=True)\n",
    "waterarea = waterarea * area_per_pixel\n",
    "waterarea = waterarea.persist()   # trigger the computation on the Dask workers\n",
    "\n",
    "### Plot\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12, 4))\n",
    "waterarea.plot()\n",
    "axes.set_xlabel(\"Date\")\n",
    "axes.set_ylabel(\"Waterbody area (km$^2$)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average NDCI\n",
    "\n",
    "The next cell computes the average NDCI for each time step using the filtered data set. This means that we're only tracking the NDCI in waterbody areas, and not on land.\n",
    "\n",
    "To make the summary plot, we calculate NDCI across all pixels; this allows us to track overall changes in NDCI, but doesn't tell us where the increase occured within the waterbody (this is covered in the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average NDCI\n",
    "average_ndci = ds_s2_water.NDCI.mean(dim=[\"x\", \"y\"], skipna=True)\n",
    "average_ndci = average_ndci.persist()   # trigger the computation on the Dask workers\n",
    "\n",
    "### Plot\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12, 4))\n",
    "average_ndci.plot()\n",
    "axes.set_xlabel(\"Date\")\n",
    "axes.set_ylabel(\"Average NDCI\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined indices plot\n",
    "\n",
    "The cell below combines the total water area and average NDCI time series we generated above into a single summary plot. Notice that Python can be used to build highly-customised plots such as the stripe plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(16, 5))\n",
    "\n",
    "### Colour map and normalisation\n",
    "# NDCI is plotted on a scale of -0.1 to 0.3, so normalise the colour map accordingly\n",
    "min_ndci_scale = -0.05\n",
    "max_ndci_scale = 0.2\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "normal = plt.Normalize(vmin=min_ndci_scale, vmax=max_ndci_scale)\n",
    "\n",
    "### Store dataset dates as numbers (for ease of plotting)\n",
    "dates = matplotlib.dates.date2num(ds_s2_water.time.values)\n",
    "\n",
    "### Waterbody area line\n",
    "axes.plot_date(x=dates, y=waterarea, color=\"black\", linestyle=\"-\", marker=\"\")\n",
    "\n",
    "### NDCI as filled stripes\n",
    "# Fill in the plot by looping over the possible threshold values and filling\n",
    "# the areas that are greater than the threshold\n",
    "color_vals = np.linspace(0, 1, 100)\n",
    "threshold_vals = np.linspace(min_ndci_scale, max_ndci_scale, 100)\n",
    "for ii,thresh in enumerate(threshold_vals):\n",
    "    im = axes.fill_between( dates, 0, waterarea, where=(average_ndci>=thresh),\n",
    "                            norm=normal, facecolor=cmap(color_vals[ii]), alpha=1 )\n",
    "\n",
    "### Colour bar\n",
    "cax, _ = matplotlib.colorbar.make_axes(axes)\n",
    "cb2 = matplotlib.colorbar.ColorbarBase(cax, cmap=cmap, norm=normal)\n",
    "cb2.set_label(\"NDCI\")\n",
    "\n",
    "### Titles and labels\n",
    "axes.set_xlabel(\"Date\")\n",
    "axes.set_ylabel(\"Waterbody area (km$^2$)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial plots\n",
    "\n",
    "## NDCI plots\n",
    "\n",
    "While the summary plot above is useful at a glance, it can be interesting to see the full spatial picture at times where the NDCI is especially low or high. In the cell below, we first identify the time indices corresponding to the minimum and maximum NDCI values in the time series. We can then use these time indices to extract and plot the data of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndci_dates = ds_s2.time.values\n",
    "min_ndci_date = ndci_dates[ np.nanargmin(average_ndci.values) ]\n",
    "max_ndci_date = ndci_dates[ np.nanargmax(average_ndci.values) ]\n",
    "\n",
    "# Selected dates as Xarray data array (to select dates from the dataset)\n",
    "time_xr = xr.DataArray([min_ndci_date, max_ndci_date], dims=[\"time\"])\n",
    "\n",
    "### NDCI plots\n",
    "ds_s2_water.NDCI.sel(time=time_xr).plot.imshow( \"x\", \"y\", col=\"time\", cmap=cmap,\n",
    "                                                vmin=min_ndci_scale, vmax=max_ndci_scale,\n",
    "                                                col_wrap=2, robust=True, figsize=(12, 5) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could also manually select two specific dates to display. The code below defines two useful functions: `closest_date` will find the date in a list of dates closest to any given date, and `date_index` will return the position of a particular date in a list of dates. These functions are useful for selecting images to compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_date(list_of_dates, desired_date):\n",
    "    return min( list_of_dates, key=lambda x: abs(x-np.datetime64(desired_date)) )\n",
    "\n",
    "def date_index(list_of_dates, known_date):\n",
    "    return (np.where(list_of_dates == known_date)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define and plot two dates to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dates to view\n",
    "date_1 = time[0]  # or change to a significant date from plots above, \"yyyy-mm-dd\"\n",
    "date_2 = time[1]  # or change to a significant date from plots above, \"yyyy-mm-dd\"\n",
    "\n",
    "### Closest date available\n",
    "closest_date_1 = closest_date(ds_s2.time.values, date_1)\n",
    "closest_date_2 = closest_date(ds_s2.time.values, date_2)\n",
    "\n",
    "# Selected dates as Xarray data array (to select dates from the dataset)\n",
    "time_xr = xr.DataArray([closest_date_1, closest_date_2], dims=[\"time\"])\n",
    "\n",
    "### NDCI plots\n",
    "ds_s2_water.NDCI.sel(time=time_xr).plot.imshow( \"x\", \"y\", col=\"time\", cmap=cmap,\n",
    "                                                vmin=min_ndci_scale, vmax=max_ndci_scale,\n",
    "                                                col_wrap=2, robust=True, figsize=(12, 5) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True colour display\n",
    "\n",
    "And finally, the cell below displays the true-colour images for the selected dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index of closest dates\n",
    "closest_date_1_idx = date_index(ds_s2.time.values, closest_date_1)\n",
    "closest_date_2_idx = date_index(ds_s2.time.values, closest_date_2)\n",
    "\n",
    "### True colour plots\n",
    "image_array = ds_s2[['red', 'green', 'blue']].isel(time=[closest_date_1_idx, closest_date_2_idx]).to_array()\n",
    "image_array.plot.imshow(robust=True, col='time', col_wrap=3, size=6, aspect=ds_s2.x.shape[0]/ds_s2.y.shape[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### End notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
