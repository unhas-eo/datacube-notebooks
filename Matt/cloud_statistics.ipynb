{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# UNHAS: Cloud coverage and valid pixels statistics\n",
    "\n",
    "**Contents**\n",
    "\n",
    "  - [Overview](#Overview)\n",
    "  - [Notebook setup](#Notebook-setup)\n",
    "    - [Load packages](#Load-packages)\n",
    "    - [Dask](#Dask)\n",
    "  - [Satellite data](#Satellite-data)\n",
    "    - [Platforms and products](#Platforms-and-products)\n",
    "    - [Datacube query](#Datacube-query)\n",
    "  - [Data clean up](#Data-clean-up)\n",
    "    - [Raw data display](#Raw-data-display)\n",
    "    - [Valid pixel mask](#Valid-pixel-mask)\n",
    "    - [Clear pixel mask](#Clear-pixel-mask)\n",
    "  - [Statistics](#Statistics)\n",
    "    - [Tabular results for each date](#Tabular-results-for-each-date)\n",
    "    - [Time series plot](#Time-series-plot)\n",
    "    - [Spatial plot of clear pixel percentage](#Spatial-plot-of-clear-pixel-percentage)\n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "This notebook explores a Landsat 8 dataset around Lake Tempe, Sulawesi, Indonesia and generates various statistics related to the percentage of valid and/or cloud-free pixels over the selected time window. This represents potential information of interest to inform subsequent analyses applied the dataset. For example, if there are extensive clouds for a given season, it may significantly impact a resulting mosaic product or a derived dataset of index values. Another example is that a user may want to find a single date when there are few clouds to assess land features.\n",
    "\n",
    "This notebook considers two types of \"valid pixel\" statistics. On one hand, we define _valid pixels_ as those pixels that have valid data associated with them &ndash; these valid observations might include clear land and/or water, as well as clouds, cloud shadows, saturated pixels, etc. It may be that part of an image in a time slice over a given region of interest has not been imaged by the sensor, in which case the corresponding pixels are _not_ included in the set of _valid pixels_.\n",
    "\n",
    "On the other hand, of those pixels providing valid observations, some may be affected by acquisition quality issues such as clouds, saturation, etc. and thus do not provide clear observations of the land and/or water over the region of interest. The pixels _not_ affected by these types of quality issues are here labelled as _clear pixels_.\n",
    "\n",
    "This notebook is adapted from a [CEOS Open Data Cube Notebook](https://github.com/ceos-seo/data_cube_notebooks/blob/master/notebooks/DCAL/DCAL_Cloud_Statistics.ipynb) example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Notebook setup\n",
    "\n",
    "## Load packages\n",
    "\n",
    "In the next cell, we load the key Python packages and supporting functions required for the subsequent analysis, and then connect to the EASI data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "### System\n",
    "import sys\n",
    "import re\n",
    "\n",
    "### Datacube \n",
    "import datacube\n",
    "from datacube.utils import masking  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/masking.py\n",
    "from odc.algo import enum_to_bool   # https://github.com/opendatacube/odc-tools/blob/develop/libs/algo/odc/algo/_masking.py\n",
    "\n",
    "### Data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "### Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### EASI tools\n",
    "sys.path.append('/home/jovyan/hub-notebooks/scripts')\n",
    "import notebook_utils\n",
    "from app_utils import display_map\n",
    "from notebook_utils import initialize_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"cloud_stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dask\n",
    "\n",
    "This notebook will make use of a relatively large dataset of remote sensing imagery, so rather than loading the data into the system memory, we will make use of Dask to load up and process the dataset. Dask is a Python library for distributed (parallel) processing, which allows us to work on very large array or datacube objects that would otherwise not fit in the memory of any single compute node. Dask relies on the creation of a Dask Gateway (or local) cluster with associated scheduler, which is done in the following cell through the initialize_dask helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster, client = notebook_utils.initialize_dask(workers=(1,2), use_gateway=True, wait=False)\n",
    "display(cluster if cluster else client)\n",
    "notebook_utils.localcluster_dashboard(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dashboard` link provided above (`Client` section) can be used to monitor the status of the Dask cluster and associated processing tasks during the execution of various cells in the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Satellite data\n",
    "\n",
    "In this section, we select and load up the relevant dataset of Landsat-8 data for subsequent analyses in this notebook.\n",
    "\n",
    "## Platforms and products\n",
    "\n",
    "Users can refer to the [ODC Explorer](https://explorer.csiro.easi-eo.solutions/products/ga_ls8c_ard_3/extents) in order to investigate the various datasets on the current EASI deployment, as well as their respective temporal and spatial extents. From within the notebook, we can also load a list of all available products using the `dc.list_products` function, to the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_info = dc.list_products()\n",
    "products_info[:10]   # first 10 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subsequently filter the results to only display the products related to a given satellite sensor. Here, we are specifically interested in products related to Landsat-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of list_products() changed between datacube versions 1.8.4 and 1.8.6\n",
    "# See https://github.com/opendatacube/datacube-core/blob/develop/datacube/api/core.py#L93\n",
    "# \"platform\" is not included. Would need to write a custom \"list_products\".\n",
    "# Use the product name instead.\n",
    "\n",
    "### Available Landsat-8 products \n",
    "pattern = re.compile('ls8')\n",
    "selected_products = [p for p in products_info.index if pattern.search(p)]\n",
    "products_info.loc[selected_products]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Several of these datasets are derived products such as fractional cover (`fc`), geomedian, barest earth dataset, etc. In this notebook, we will use the Landsat-8 data labelled `ga_ls8c_ard_3`. We can further list the various spectral (and ancillary) measurements for this dataset, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = 'ga_ls8c_ard_3'\n",
    "product_measurements = dc.list_measurements().loc[product]\n",
    "product_measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further below, we will mask the data to identify the pixels containing valid information (i.e. not `no-data`), as well as those pixels not affected by clouds, cloud shadow, etc. (i.e. clear pixels). Information regarding the quality of each pixel is provided in the `oa_fmask` data layer, which can be investigated in more detail as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_layer = 'oa_fmask'\n",
    "product_measurements.loc[QA_layer]['flags_definition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datacube query\n",
    "\n",
    "We can now create a query dictionary that will be used to load up the desired dataset from the EASI data cube. We will here load up the data in its native projection, which can be determined by making use of the `mostcommon_crs` function. Also, in addition to the pixel quality band (`oa_fmask`), we select the red, green and blue Landsat-8 bands to be able to plot the data in a true-colour display later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = { 'product': product,\n",
    "          'lat': (-42.6, -43.2),           # Hobart, Tasmania\n",
    "          'lon': (146.9, 147.9),\n",
    "          'measurements': ['nbart_red', 'nbart_green', 'nbart_blue', 'oa_fmask'],\n",
    "          'time': ('2020-01-01', '2020-07-01'),\n",
    "          'group_by': 'solar_day' }        # scene ordering\n",
    "\n",
    "### Dataset's native projection\n",
    "native_crs = notebook_utils.mostcommon_crs(dc, query)\n",
    "print(f\"The dataset's native CRS is: {native_crs}\")\n",
    "\n",
    "query.update({ 'output_crs': native_crs,   # EPSG code\n",
    "               'resolution': (30, 30),     # target resolution\n",
    "               'dask_chunks': {'x': 512, 'y': 512} })   # Dask processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "For insight, the selected spatial region can be visualised using the `display_map` utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "display_map(x=query['lon'], y=query['lat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now load up the data, using the `query` parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dc.load(**query)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Data clean up\n",
    "\n",
    "Typically, a first step to perform after loading a dataset of satellite imagery is to clean up the data. In particular, this involves the removal (masking out) of pixels affected by clouds, cloud shadows, etc., as well as those pixels identified as having no valid data in specific bands and/or time slices.\n",
    "\n",
    "## Raw data display\n",
    "\n",
    "We start by creating a true-colour display of some time slices (here, the first three time indices) from the un-processed dataset. This will allow us to better understand the various masks that will be produced in order to clean up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### True-colour display\n",
    "image_array = data[['nbart_red', 'nbart_green', 'nbart_blue']].isel(time=range(3)).to_array()\n",
    "image_array.plot.imshow(robust=True, col='time', col_wrap=3, figsize=(11,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these plots, we can clearly identify pixels with no valid data (shown in black &ndash; typically located outside a scene of satellite imagery in some time slices), as well as valid pixels affected by clouds (bright pixels). The rest are mostly clean pixels, i.e. pixels with valid data that are not affected by quality issues such as clouds, shadows, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid pixel mask\n",
    "\n",
    "First, we can create a mask layer to identify the `no-data` pixels, i.e. invalid pixels. This mask can be used to keep only those pixels corresponding to valid data, which will here correspond to clear observations, as well as clouds, shadows, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Valid (not 'no-data') masks, for all bands\n",
    "data_valid_mask = masking.valid_data_mask(data)\n",
    "data_valid_mask = data_valid_mask.persist()\n",
    "\n",
    "data_valid_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `data_valid_mask` has a separate layer for each of the satellite bands: indeed, it could be that different measurements (i.e. bands, such as `nbart_red`, `nbart_blue`, etc.) each have different pixels flagged as `no-data`, perhaps as a result of differing data acquisition or pre-processing procedures. \n",
    "\n",
    "Here, we will combine these layer-specific masks into an overall mask identifying the pixels that have valid data in all bands in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mask = data_valid_mask['nbart_red'] & data_valid_mask['nbart_green'] & data_valid_mask['nbart_blue'] & data_valid_mask['oa_fmask']\n",
    "valid_mask = valid_mask.persist()\n",
    "valid_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the first three masks in the `valid_mask` time series, corresponding to the same dates as the raw data plots displayed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mask[:3].plot(col='time', col_wrap=3, figsize=(13,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen here, the `valid_mask` layers clearly identify those pixels in the dataset containing valid data (yellow pixels), as opposed to those with missing data (purple pixels)\n",
    "\n",
    "## Clear pixel mask\n",
    "\n",
    "Based on the pixel quality flag (`oa_fmask` in the current dataset), we can now derive a mask of clear observations for each time slice, identifying those pixels not affected by clouds, cloud shadows, etc. Because such phenomena simultaneously affect all bands in the dataset, there will only be one such mask per time slice (for all layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### L2_FLAGS mask\n",
    "clear_pixel_flags = {'valid', 'water'}   # i.e. exclude 'nodata', 'cloud', 'shadow', 'snow'\n",
    "\n",
    "clear_mask = enum_to_bool(data[QA_layer], clear_pixel_flags)\n",
    "clear_mask.name = \"clear_mask\"\n",
    "clear_mask = clear_mask.persist()\n",
    "\n",
    "clear_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's plot the first three masks in the `clear_mask` time series, and compare it with the previous plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mask[:3].plot(col='time', col_wrap=3, figsize=(13,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the pixels identified as providing clear observations (as per the `clear_mask` layers) are a subset of the pixels found to have valid data (`valid_mask`). Other valid pixels affected by pixel quality issues (here, mainly clouds) have been removed from the clear pixel mask (`clear_mask`).\n",
    "\n",
    "# Statistics\n",
    "\n",
    "Based on these datasets derived from the raw Landsat-8 imagery, we can now calculate some statistics of interest that can be potentially used to inform further analyses of the data.\n",
    "\n",
    "## Tabular results for each date\n",
    "\n",
    "For each time slice in the time series, we can calculate the percentage of valid pixels, i.e. the sum of all valid pixels (not `no-data`) divided by the total number of pixels in the image. Because each valid pixel is represented by a value of `1.0` in the `valid_mask` object (and a value of `0.0` otherwise), we can simply use the `mean` function to implement this operation. The same can be done for the percentage of clear pixels (i.e. not affected by clouds, shadows, no-data, etc.).\n",
    "\n",
    "Note that these values represent the percentage of clear (respectively valid) pixels as a proportion of the _total number of pixels over the whole imaging region_, as opposed to the clear percentage representing the fraction of clear pixels _as a percentage of valid pixels_. \n",
    "\n",
    "And because clear pixels are a subset of valid pixel, the clear pixel percentage (for each image, as well as overall) should always be lower than the valid pixel percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Valid pixel percentage in each time slice\n",
    "count_valid = valid_mask.sum(dim=['x','y']).values\n",
    "percentage_valid = np.round( valid_mask.mean(dim=['x','y']).values*100.0, 2)\n",
    "\n",
    "### Clear pixel percentage in each time slice\n",
    "count_clear = clear_mask.sum(dim=['x','y']).values\n",
    "percentage_clear = np.round( clear_mask.mean(dim=['x','y']).values*100.0, 2)\n",
    "\n",
    "### Tabular results\n",
    "tmp = {\"date\": data.time.values,\n",
    "       \"valid_count\": count_valid, \n",
    "       \"valid_percentage\": percentage_valid,\n",
    "       \"clear_count\": count_clear, \n",
    "       \"clear_percentage\": percentage_clear }\n",
    "valid_DF = pd.DataFrame(data=tmp)\n",
    "valid_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series plot\n",
    "\n",
    "The table above contains the numeric results related to the metrics of interest (clear and valid pixel percentages). We can also show these same results in a time series plot, where the percentages are displayed in a bar plot with different colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(valid_DF[\"date\"].values, valid_DF[\"valid_percentage\"].values, 1.0)\n",
    "plt.bar(valid_DF[\"date\"].values, valid_DF[\"clear_percentage\"].values, 1.0, color='orange')\n",
    "plt.title(\"Percentage of valid pixels (blue) and clear pixels (orange)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial plot of clear pixel percentage\n",
    "\n",
    "Finally, we can also plot of overall percentage of clear observations (this time, as a _fraction of valid observations_) for each pixel over the whole time series. The result is a map indicating which regions are more or less affected by data quality issues, and thus providing information on whether subsequent analyses might be affected by the lack of clear observations in certain areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Valid & clear pixel counts (temporal aggregation)\n",
    "valid_sum = valid_mask.sum(dim='time')\n",
    "clear_sum = clear_mask.sum(dim='time')\n",
    "\n",
    "### Fraction of clear pixels to valid pixels\n",
    "percentage_clear = 100.0 * clear_sum / valid_sum\n",
    "percentage_clear = percentage_clear.where(percentage_clear<=100.0)   # remove some potential data glitches...\n",
    "percentage_clear = percentage_clear.persist()\n",
    "\n",
    "percentage_clear.plot(figsize=(12,10), cmap=\"RdYlGn\")\n",
    "plt.title('Percentage of clear observations (as fraction of valid pixels)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further overall (whole-of-time-series) statistics can also be provided as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall percentage of valid pixels: {:.2%}\".format(valid_mask.mean().values))\n",
    "print(\"Overall percentage of clear pixels: {:.2%}\".format(clear_mask.mean().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(\"Number of pixels which have no clear data:\", (percentage_clear==0).sum().values)\n",
    "print(\"Total number of pixels in each time slice:\", data.dims['x']*data.dims['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### End notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
